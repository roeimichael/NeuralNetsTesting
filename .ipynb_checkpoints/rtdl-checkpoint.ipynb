{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6550c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements:\n",
    "!pip install rtdl\n",
    "!pip install libzero==0.0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f761ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "import numpy as np\n",
    "import rtdl\n",
    "import scipy.special\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dee770",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "# Docs: https://yura52.github.io/delu/0.0.4/reference/api/zero.improve_reproducibility.html\n",
    "zero.improve_reproducibility(seed=123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc60877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "class CSVDataset_rtdl(Dataset):\n",
    "    def __init__(self, path_train, path_test, targets_path, starting_date):\n",
    "        df_train_num, df_train_cat = self.read_and_preprocess(path_train)\n",
    "        df_test_num, df_test_cat = self.read_and_preprocess(path_test)\n",
    "        self.targets_df = read_csv(targets_path, index_col=0)\n",
    "\n",
    "        self.x_train_num, self.x_train_cat, self.y_train = self.split_features_labels(df_train_num, df_train_cat, starting_date + 1)\n",
    "        self.x_test_num, self.x_test_cat, self.y_test = self.split_features_labels(df_test_num, df_test_cat, starting_date + 2)\n",
    "\n",
    "        # Normalize the training and test data using StandardScaler for numerical data only\n",
    "        scaler = StandardScaler()\n",
    "        self.x_train_num = scaler.fit_transform(self.x_train_num)\n",
    "        self.x_test_num = scaler.transform(self.x_test_num)\n",
    "\n",
    "        # Calculate cardinalities and d_token\n",
    "        self.cardinalities = self.calculate_cardinalities(df_train_cat, df_test_cat)\n",
    "\n",
    "\n",
    "    def read_and_preprocess(self, path):\n",
    "        df = read_csv(path, header=None)\n",
    "        df = df.iloc[1:, 1:]\n",
    "\n",
    "        # Separate categorical and numerical data\n",
    "        df_cat = df.loc[:, df.nunique() <= 2] # consider binary and categorical columns\n",
    "        df_num = df.loc[:, df.nunique() > 2]  # consider numeric columns\n",
    "\n",
    "        return df_num, df_cat\n",
    "\n",
    "    def split_features_labels(self, df_num, df_cat, target_date):\n",
    "        x_num = df_num.values.astype('float32')\n",
    "        x_cat = df_cat.values.astype('int64')  # categorical values are converted to int\n",
    "\n",
    "        y = self.targets_df.values[:, target_date]\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "        y = y.astype('float32').reshape((len(y), 1))\n",
    "\n",
    "        return x_num, x_cat, y\n",
    "\n",
    "    def calculate_cardinalities(self, df_train_cat, df_test_cat):\n",
    "        # Combine train and test categorical dataframes to get total unique values for each category\n",
    "        df_total_cat = pd.concat([df_train_cat, df_test_cat], axis=0)\n",
    "        # Calculate the cardinalities\n",
    "        cardinalities = [df_total_cat[col].nunique() for col in df_total_cat.columns]\n",
    "        return cardinalities\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_train_num)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.x_train_num[idx], self.x_train_cat[idx], self.y_train[idx]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c11d0e87",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'AAPL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20452/3847261571.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;31m# Prepare the data using the function defined above\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mtrain_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcardinalities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstarting_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20452/3847261571.py\u001b[0m in \u001b[0;36mprepare_data\u001b[1;34m(path_train, path_test, train_path, starting_date)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstarting_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCSVDataset_rtdl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstarting_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mcardinalities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcardinalities\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\neural_testing\\CSVDataset_rtdl.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_train, path_test, targets_path, starting_date)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtargets_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_train_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_features_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstarting_date\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_test_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_features_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstarting_date\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\neural_testing\\CSVDataset_rtdl.py\u001b[0m in \u001b[0;36msplit_features_labels\u001b[1;34m(self, df_num, df_cat, target_date)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msplit_features_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_cat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mx_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_num\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mx_cat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_cat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int64'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# categorical values are converted to int\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'AAPL'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rtdl\n",
    "import scipy.special\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import zero\n",
    "from CSVDataset_rtdl import CSVDataset_rtdl\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from models.rtdl_wrapper import Model\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "zero.improve_reproducibility(seed=123456)\n",
    "\n",
    "def prepare_data(path_train, path_test, train_path, starting_date):\n",
    "    dataset = CSVDataset_rtdl(path_train, path_test, train_path, starting_date)\n",
    "    cardinalities = dataset.cardinalities\n",
    "\n",
    "    x_train_tensor = torch.from_numpy(dataset.x_train).float()\n",
    "    y_train_tensor = torch.from_numpy(dataset.y_train).float()\n",
    "    x_test_tensor = torch.from_numpy(dataset.x_test).float()\n",
    "    y_test_tensor = torch.from_numpy(dataset.y_test).float()\n",
    "\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "\n",
    "    return train_dl, test_dl, cardinalities\n",
    "\n",
    "# The main function for running the training process\n",
    "\n",
    "with open('./data/dates.txt', 'r') as fp:\n",
    "    dates = [line.strip() for line in fp.readlines()]\n",
    "starting_date = 466\n",
    "\n",
    "path_train = f'./data/dates1.5/{dates[starting_date]}.csv'\n",
    "path_test = f'./data/dates1.5/{dates[starting_date + 1]}.csv'\n",
    "train_path = './data/Targets1.5.csv'\n",
    "next_day_data_path = f'./data/dates1.5/{dates[starting_date + 2]}.csv'\n",
    "\n",
    "# Prepare the data using the function defined above\n",
    "train_dl, test_dl, cardinalities = prepare_data(path_train, path_test, train_path, starting_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e4fb7",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! NOTE !!! The dataset splits, preprocessing and other details are\n",
    "# significantly different from those used in the\n",
    "# paper \"Revisiting Deep Learning Models for Tabular Data\",\n",
    "# so the results will be different from the reported in the paper.\n",
    "\n",
    "dataset = sklearn.datasets.fetch_california_housing()\n",
    "task_type = 'regression'\n",
    "\n",
    "# dataset = sklearn.datasets.fetch_covtype()\n",
    "# task_type = 'multiclass'\n",
    "\n",
    "assert task_type in ['binclass', 'multiclass', 'regression']\n",
    "\n",
    "X_all = dataset['data'].astype('float32')\n",
    "y_all = dataset['target'].astype('float32' if task_type == 'regression' else 'int64')\n",
    "if task_type != 'regression':\n",
    "    y_all = sklearn.preprocessing.LabelEncoder().fit_transform(y_all).astype('int64')\n",
    "n_classes = int(max(y_all)) + 1 if task_type == 'multiclass' else None\n",
    "\n",
    "X = {}\n",
    "y = {}\n",
    "X['train'], X['test'], y['train'], y['test'] = sklearn.model_selection.train_test_split(\n",
    "    X_all, y_all, train_size=0.8\n",
    ")\n",
    "X['train'], X['val'], y['train'], y['val'] = sklearn.model_selection.train_test_split(\n",
    "    X['train'], y['train'], train_size=0.8\n",
    ")\n",
    "\n",
    "# not the best way to preprocess features, but enough for the demonstration\n",
    "preprocess = sklearn.preprocessing.StandardScaler().fit(X['train'])\n",
    "X = {\n",
    "    k: torch.tensor(preprocess.transform(v), device=device)\n",
    "    for k, v in X.items()\n",
    "}\n",
    "y = {k: torch.tensor(v, device=device) for k, v in y.items()}\n",
    "\n",
    "# !!! CRUCIAL for neural networks when solving regression problems !!!\n",
    "if task_type == 'regression':\n",
    "    y_mean = y['train'].mean().item()\n",
    "    y_std = y['train'].std().item()\n",
    "    y = {k: (v - y_mean) / y_std for k, v in y.items()}\n",
    "else:\n",
    "    y_std = y_mean = None\n",
    "\n",
    "if task_type != 'multiclass':\n",
    "    y = {k: v.float() for k, v in y.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6846056",
   "metadata": {},
   "source": [
    "### Model\n",
    "Carefully read the comments and uncomment the code for the model you want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_out = n_classes or 1\n",
    "\n",
    "# model = rtdl.MLP.make_baseline(\n",
    "#     d_in=X_all.shape[1],\n",
    "#     d_layers=[128, 256, 128],\n",
    "#     dropout=0.1,\n",
    "#     d_out=d_out,\n",
    "# )\n",
    "# lr = 0.001\n",
    "# weight_decay = 0.0\n",
    "\n",
    "# model = rtdl.ResNet.make_baseline(\n",
    "#     d_in=X_all.shape[1],\n",
    "#     d_main=128,\n",
    "#     d_intermidiate=256,\n",
    "#     dropout_first=0.2,\n",
    "#     dropout_second=0.0,\n",
    "#     n_blocks=2,\n",
    "#     d_out=d_out,\n",
    "# )\n",
    "# lr = 0.001\n",
    "# weight_decay = 0.0\n",
    "\n",
    "model = rtdl.FTTransformer.make_default(\n",
    "    n_num_features=X_all.shape[1],\n",
    "    cat_cardinalities=None,\n",
    "    last_layer_query_idx=[-1],  # it makes the model faster and does NOT affect its output\n",
    "    d_out=d_out,\n",
    ")\n",
    "\n",
    "# === ABOUT CATEGORICAL FEATURES ===\n",
    "# IF you use MLP, ResNet or any other simple feed-forward model (NOT transformer-based model)\n",
    "# AND there are categorical features\n",
    "# THEN you have to implement a wrapper that handles categorical features.\n",
    "# The example below demonstrates how it can be achieved using rtdl.CategoricalFeatureTokenizer.\n",
    "# ==================================\n",
    "# 1. When you have both numerical and categorical features, you should prepare you data like this:\n",
    "#    (X_num<float32>, X_cat<int64>) instead of X<float32>\n",
    "#    Each column in X_cat should contain values within the range from 0 to <(the number of unique values in column) - 1>;\n",
    "#    use sklean.preprocessing.OrdinalEncoder to achieve this;\n",
    "# 2. Prepare a list of so called \"cardinalities\":\n",
    "#    cardinalities[i] = <the number of unique values of the i-th categorical feature>\n",
    "# 3. See the commented example below and adapt it for your needs.\n",
    "#\n",
    "# class Model(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         n_num_features: int,\n",
    "#         cat_tokenizer: rtdl.CategoricalFeatureTokenizer,\n",
    "#         mlp_kwargs: Dict[str, Any],\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.cat_tokenizer = cat_tokenizer\n",
    "#         self.model = rtdl.MLP.make_baseline(\n",
    "#             d_in=n_num_features + cat_tokenizer.n_tokens * cat_tokenizer.d_token,\n",
    "#             **mlp_kwargs,\n",
    "#         )\n",
    "#\n",
    "#     def forward(self, x_num, x_cat):\n",
    "#         return self.model(\n",
    "#             torch.cat([x_num, self.cat_tokenizer(x_cat).flatten(1, -1)], dim=1)\n",
    "#         )\n",
    "#\n",
    "# model = Model(\n",
    "#     # `None` means \"Do not transform numerical features\"\n",
    "#     # `d_token` is the size of embedding for ONE categorical feature\n",
    "#     X_num_all.shape[1],\n",
    "#     rtdl.CategoricalFeatureTokenizer(cardinalities, d_token, True, 'uniform'),\n",
    "#     mlp_kwargs,\n",
    "# )\n",
    "# Then the model should be used as `model(x_num, x_cat)` instead of of `model(x)`.\n",
    "\n",
    "model.to(device)\n",
    "optimizer = (\n",
    "    model.make_default_optimizer()\n",
    "    if isinstance(model, rtdl.FTTransformer)\n",
    "    else torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    ")\n",
    "loss_fn = (\n",
    "    F.binary_cross_entropy_with_logits\n",
    "    if task_type == 'binclass'\n",
    "    else F.cross_entropy\n",
    "    if task_type == 'multiclass'\n",
    "    else F.mse_loss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87be57f",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cfe082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(x_num, x_cat=None):\n",
    "    if isinstance(model, rtdl.FTTransformer):\n",
    "        return model(x_num, x_cat)\n",
    "    elif isinstance(model, (rtdl.MLP, rtdl.ResNet)):\n",
    "        assert x_cat is None\n",
    "        return model(x_num)\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f'Looks like you are using a custom model: {type(model)}.'\n",
    "            ' Then you have to implement this branch first.'\n",
    "        )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(part):\n",
    "    model.eval()\n",
    "    prediction = []\n",
    "    for batch in zero.iter_batches(X[part], 1024):\n",
    "        prediction.append(apply_model(batch))\n",
    "    prediction = torch.cat(prediction).squeeze(1).cpu().numpy()\n",
    "    target = y[part].cpu().numpy()\n",
    "\n",
    "    if task_type == 'binclass':\n",
    "        prediction = np.round(scipy.special.expit(prediction))\n",
    "        score = sklearn.metrics.accuracy_score(target, prediction)\n",
    "    elif task_type == 'multiclass':\n",
    "        prediction = prediction.argmax(1)\n",
    "        score = sklearn.metrics.accuracy_score(target, prediction)\n",
    "    else:\n",
    "        assert task_type == 'regression'\n",
    "        score = sklearn.metrics.mean_squared_error(target, prediction) ** 0.5 * y_std\n",
    "    return score\n",
    "\n",
    "\n",
    "# Create a dataloader for batches of indices\n",
    "# Docs: https://yura52.github.io/delu/reference/api/zero.data.IndexLoader.html\n",
    "batch_size = 256\n",
    "train_loader = zero.data.IndexLoader(len(X['train']), batch_size, device=device)\n",
    "\n",
    "# Create a progress tracker for early stopping\n",
    "# Docs: https://yura52.github.io/delu/reference/api/zero.ProgressTracker.html\n",
    "progress = zero.ProgressTracker(patience=100)\n",
    "\n",
    "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e51ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "report_frequency = len(X['train']) // batch_size // 5\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    for iteration, batch_idx in enumerate(train_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        x_batch = X['train'][batch_idx]\n",
    "        y_batch = y['train'][batch_idx]\n",
    "        loss = loss_fn(apply_model(x_batch).squeeze(1), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if iteration % report_frequency == 0:\n",
    "            print(f'(epoch) {epoch} (batch) {iteration} (loss) {loss.item():.4f}')\n",
    "\n",
    "    val_score = evaluate('val')\n",
    "    test_score = evaluate('test')\n",
    "    print(f'Epoch {epoch:03d} | Validation score: {val_score:.4f} | Test score: {test_score:.4f}', end='')\n",
    "    progress.update((-1 if task_type == 'regression' else 1) * val_score)\n",
    "    if progress.success:\n",
    "        print(' <<< BEST VALIDATION EPOCH', end='')\n",
    "    print()\n",
    "    if progress.fail:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7813c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
